# -*- coding: utf-8 -*-
"""ML_Offline3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P_Ylg8BdNcbeOhsjKSCWsAHU_3mokhi9
"""

import numpy as np 
import matplotlib.pyplot as plt

data = np.genfromtxt('/content/drive/My Drive/ML_Offline_3/data.txt')

X_train = data

from sklearn.preprocessing import StandardScaler
X_std = StandardScaler().fit_transform(X_train)
print(X_std.shape)

import numpy as np
cov_mat = np.cov(X_std.T)
print('Covariance matrix \n%s' %cov_mat)

eig_vals, eig_vecs = np.linalg.eig(cov_mat)

print('Eigenvectors \n%s' %eig_vecs)
print('\nEigenvalues \n%s' %eig_vals)

# Make a list of (eigenvalue, eigenvector) tuples
eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]

# Sort the (eigenvalue, eigenvector) tuples from high to low
eig_pairs.sort(key=lambda x: x[0], reverse=True)

# Visually confirm that the list is correctly sorted by decreasing eigenvalues
print('Eigenvalues in descending order:')
for i in eig_pairs:
    print(i[0])

matrix_w = np.hstack((eig_pairs[0][1].reshape(-1,1),
                      eig_pairs[1][1].reshape(-1,1)))

print('Matrix W:\n', matrix_w)

convertedInput = np.dot(X_std,matrix_w)
print(convertedInput)

import matplotlib.pyplot as plt 
import numpy as np 

plt.xlabel('PC1')
plt.ylabel('PC2')

x = convertedInput[:, 0]
y = convertedInput[:, 1]

plt.scatter(x, y, s=20, facecolors='none', edgecolors='b')
pca_plot = plt.gcf()
plt.show()

pca_plot.savefig('pca_plot.png')

#Start EM algorithm
#data = m by n
data = convertedInput

from matplotlib import pyplot
plt.hist(data, bins=100)
plt.show()

import math

#xi = n by 1
#mean = n by 1
#sigma = n by n
def guassian_probability(xi, mean, sigma):

  D = xi.shape[0]

  det_sigma = np.linalg.det(sigma)

  denom = ((2 * math.pi) ** D) * det_sigma

  denom = np.sqrt(denom)

  inv_sigma = np.linalg.inv(sigma)

  diff = (xi - mean)
  mul1 = np.dot(diff.T, inv_sigma)
  mul2 = np.dot(mul1, diff)

  pow = -0.5 * mul2

  numer = np.exp(pow)

  p = numer/denom

  #exp = (0.5 * np.dot(np.dot((xi - mean).T, np.linalg.inv(sigma)), (xi-mean))) + 0.00000001
  #exp = np.exp(-exp)
  #print("Exp: " + str(exp))
  #p = (1/(np.sqrt(((2 * math.pi)**D) * np.linalg.det(sigma)))) * exp
  #print("p: " + str(p))
  return p

def calculate_cov(X, mean):
  m = X.shape[0]
  mean = mean.reshape(mean.shape[0], -1) # n by 1
  mean = mean.T # 1 by n
  #print(mean.shape)
  diff = X - mean # should broadcast. m by n - 1 by n = m by n
  #print(diff.shape)
  #print(X)
  #print(mean)
  #print(diff)
  cov = np.dot(diff.T, diff)
  cov = (1/m) * cov
  return cov

np.random.seed(1)
def initialize_params(X, K=4):
  n = data.shape[1]
  mean = np.zeros((K, n))
  for k in range(K):
    index = np.random.randint(1, 1000)
    mean[k] = X[index]
  sigma = np.zeros((K, n, n))
  for k in range(K):
    sigma[k] = calculate_cov(X, mean[k])
  w = np.zeros((K, 1))
  for i in range(len(w)):
    w[i] = 1/K
  return mean, sigma, w

#w = K by 1
#X = m by n
#all_means = K by n
#all_sigmas = K by n by n
def E_step(w, X, all_means, all_sigmas):
  K = len(w)
  I = X.shape[0] #m
  P = np.zeros((I, K))
  for i in range(I):
    #i iterates over every datapoint
    xi = X[i]
    xi = xi.reshape(xi.shape[0], -1) # reshapes to n by 1
    #print("Xi: " + str(xi))
    numer = []
    denom = 0
    for k in range(K):
      #k iterates over every gaussian
      wk = w[k]
      mean_k = all_means[k]
      mean_k = mean_k.reshape(mean_k.shape[0], -1) #reshapes to n by 1
      sigma_k = all_sigmas[k]
      Nk =  guassian_probability(xi, mean_k, sigma_k)
      #print("Nk: " + str(Nk))
      numer.append(wk * Nk)
      denom += wk * Nk
    
    #print("Denom: " + str(denom))

    for k in range(K):
      P[i][k] = numer[k]/denom

    
  return P

#P = m by K
#X = m by n
def M_step_iterative(P, X):
  K = P.shape[1]
  m = X.shape[0]
  n = X.shape[1]

  mean = np.zeros((K, n))
  sigma = np.zeros((K, n, n))
  w = np.zeros((K, 1))

  denom_list = []

  for k in range(K):
    numer_mean = np.zeros((n, 1))
    denom_mean = 0
    for i  in range(m):
      xi = X[i]
      xi = xi.reshape(xi.shape[0], -1) #n by 1
      numer_mean = numer_mean + (P[i][k] * xi)
      denom_mean = denom_mean + P[i][k]

    denom_list.append(denom_mean)

    mean_k = numer_mean/denom_mean
    #print(mean_k)
    mean[k] = mean_k.T

  for k in range(K):
    #mean
    mean_k = mean[k]
    mean_k = mean_k.reshape(mean_k.shape[0], -1) #n by 1

    numer_sigma = np.zeros((n, n))
    for i in range(m):
      xi = X[i]
      xi = xi.reshape(xi.shape[0], -1) #n by 1 
      diff = xi - mean_k #n by 1
      numer_sigma = numer_sigma + (P[i][k] * (np.dot(diff, diff.T))) #n by n

    sigma_k = numer_sigma/denom_list[k]
    sigma[k] = sigma_k

  return mean,sigma

#P = m by K
#X = m by n
def M_step(P, X):
  K = P.shape[1]
  n = X.shape[1]
  m = X.shape[0]
  mean = np.zeros((K, n))
  sigma = np.zeros((K, n, n))
  w = np.zeros((K, 1))

  denom_list = []
  for k in range(K):
    p_k = P[:, k]
    p_k = p_k.reshape(p_k.shape[0], -1) # shape = m by 1
    p_k = p_k.T #shape = 1 by m
    denom = np.sum(p_k)
    denom_list.append(denom)

  mean = np.dot(P.T, X)
  for k in range(K):
    mean[k] = mean[k] / denom_list[k]
    w[k] = denom_list[k]/m
  
  for k in range(K):
    p_k = P[: , k]
    p_k = p_k.reshape(p_k.shape[0], -1) # shape = m by 1
    p_k = p_k.T #shape = 1 by m

    mean_k = mean[k]
    mean_k = mean_k.reshape(mean_k.shape[0], -1) #n by 1

    diff = X.T - mean_k #broadcast; final shape n by m
    numer = p_k * diff #elementwise multiplication; broadcast; shape = n by m
    numer = np.dot(numer, diff.T) #shape = n by n
    denom = np.sum(p_k)
    sigma_k = numer/denom #shap = n by n
    sigma[k] = sigma_k

  
  return mean, sigma, w

#w = K by 1
#X = m by n
#mean = K by n
#sigma = K by n by n
def log_likelihood(w, X, mean, sigma):
  K = w.shape[0]
  m = X.shape[0]
  n = X.shape[1]
  outer_sum = 0
  for i in range(m):
    inner_sum = 0
    for k in range(K):
      mean_k = mean[k]
      mean_k = mean_k.reshape(mean_k.shape[0], -1)
      sigma_k = sigma[k]
      xi = X[i]
      xi = xi.reshape(xi.shape[0], -1)
      inner_sum += w[k] * guassian_probability(xi, mean_k, sigma_k)
      #print(inner_sum)
    outer_sum += np.log(inner_sum)
  
  return outer_sum

mean, sigma, w = initialize_params(data, K=3)

prev_ll = 0
iter = 0
for i in range(1000):
  iter += 1
  P = E_step(w, data, mean, sigma)
  mean, sigma, w = M_step(P, data)
  ll = log_likelihood(w, data, mean, sigma)
  print(ll)
  if abs(ll-prev_ll) < 0.001:
    prev_ll = ll
    break
  prev_ll = ll

def print_(str):
  print(str)
  with open("report.txt", "a") as f:
    f.write(str + "\n")

print_("Iterations: " + str(iter))
print_("\n")
print_("Final log likelihood: " + str(prev_ll))
print_("\n")
print_("Mean: ")
print_(str(mean))
print_("\n")
print_("Sigma: ")
print_(str(sigma))
print_("\n")
print_("W: ")
print_(str(w))